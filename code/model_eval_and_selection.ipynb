{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5da6277-3e1d-4b51-b41d-282a5c022e55",
   "metadata": {},
   "source": [
    "# Model Evaluation and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbafb972-70d7-4a70-94ea-8b1a624a6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9e5e04-0f4c-40d8-bf36-7f0d2db64643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/model_list_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f705f9cc-cffd-47bc-a981-b78aa1029f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_cleaning</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>test_bal_acc</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>minimal</td>\n",
       "      <td>no</td>\n",
       "      <td>0.959632</td>\n",
       "      <td>0.902055</td>\n",
       "      <td>0.956994</td>\n",
       "      <td>0.897909</td>\n",
       "      <td>{'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>minimal</td>\n",
       "      <td>no</td>\n",
       "      <td>0.968840</td>\n",
       "      <td>0.902055</td>\n",
       "      <td>0.967164</td>\n",
       "      <td>0.898785</td>\n",
       "      <td>{'logisticregression__C': 0.001, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minimal</td>\n",
       "      <td>no</td>\n",
       "      <td>0.937681</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>0.937164</td>\n",
       "      <td>0.916373</td>\n",
       "      <td>{'countvectorizer__max_features': 6700, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.12}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.956028</td>\n",
       "      <td>0.897133</td>\n",
       "      <td>0.953674</td>\n",
       "      <td>0.893344</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.969186</td>\n",
       "      <td>0.904216</td>\n",
       "      <td>0.968182</td>\n",
       "      <td>0.902283</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.974247</td>\n",
       "      <td>0.898482</td>\n",
       "      <td>0.973243</td>\n",
       "      <td>0.896592</td>\n",
       "      <td>{'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.911493</td>\n",
       "      <td>0.897470</td>\n",
       "      <td>0.910347</td>\n",
       "      <td>0.895530</td>\n",
       "      <td>{'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.927238</td>\n",
       "      <td>0.903204</td>\n",
       "      <td>0.925733</td>\n",
       "      <td>0.901447</td>\n",
       "      <td>{'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.939609</td>\n",
       "      <td>0.912985</td>\n",
       "      <td>0.938581</td>\n",
       "      <td>0.911462</td>\n",
       "      <td>{'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.942982</td>\n",
       "      <td>0.905902</td>\n",
       "      <td>0.942322</td>\n",
       "      <td>0.904932</td>\n",
       "      <td>{'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_cleaning lemmatized  train_acc  test_acc  train_bal_acc  test_bal_acc  \\\n",
       "0       minimal         no   0.959632  0.902055       0.956994      0.897909   \n",
       "1       minimal         no   0.968840  0.902055       0.967164      0.898785   \n",
       "2       minimal         no   0.937681  0.917148       0.937164      0.916373   \n",
       "3          deep         no   0.956028  0.897133       0.953674      0.893344   \n",
       "4          deep        yes   0.969186  0.904216       0.968182      0.902283   \n",
       "5          deep        yes   0.974247  0.898482       0.973243      0.896592   \n",
       "6          deep         no   0.911493  0.897470       0.910347      0.895530   \n",
       "7          deep         no   0.927238  0.903204       0.925733      0.901447   \n",
       "8          deep        yes   0.939609  0.912985       0.938581      0.911462   \n",
       "9          deep        yes   0.942982  0.905902       0.942322      0.904932   \n",
       "\n",
       "                                                                                                                                                       params  \n",
       "0        {'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.001}  \n",
       "1        {'logisticregression__C': 0.001, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None}  \n",
       "2     {'countvectorizer__max_features': 6700, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.12}  \n",
       "3  {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}  \n",
       "4        {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}  \n",
       "5   {'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "6      {'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}  \n",
       "7     {'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "8     {'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}  \n",
       "9      {'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the dataframe that collected all the best scores for each model type\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e0a685-9c45-4f7e-9a22-7431ae3f2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will now be comparing the TEST accuracy scores and TEST balanced accuracy scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b60056-962a-4249-8ba0-413e8ad3a0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>test_bal_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.948694</td>\n",
       "      <td>0.904065</td>\n",
       "      <td>0.947340</td>\n",
       "      <td>0.901866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.020366</td>\n",
       "      <td>0.006546</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.007288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.911493</td>\n",
       "      <td>0.897133</td>\n",
       "      <td>0.910347</td>\n",
       "      <td>0.893344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.938163</td>\n",
       "      <td>0.899376</td>\n",
       "      <td>0.937518</td>\n",
       "      <td>0.896922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.949505</td>\n",
       "      <td>0.902630</td>\n",
       "      <td>0.947998</td>\n",
       "      <td>0.900116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.966538</td>\n",
       "      <td>0.905481</td>\n",
       "      <td>0.964621</td>\n",
       "      <td>0.904270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.974247</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>0.973243</td>\n",
       "      <td>0.916373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_acc   test_acc  train_bal_acc  test_bal_acc\n",
       "count  10.000000  10.000000      10.000000     10.000000\n",
       "mean    0.948694   0.904065       0.947340      0.901866\n",
       "std     0.020366   0.006546       0.020200      0.007288\n",
       "min     0.911493   0.897133       0.910347      0.893344\n",
       "25%     0.938163   0.899376       0.937518      0.896922\n",
       "50%     0.949505   0.902630       0.947998      0.900116\n",
       "75%     0.966538   0.905481       0.964621      0.904270\n",
       "max     0.974247   0.917148       0.973243      0.916373"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27c0b32-5c17-4331-9281-fb5f644f1be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_cleaning</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>test_bal_acc</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minimal</td>\n",
       "      <td>no</td>\n",
       "      <td>0.937681</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>0.937164</td>\n",
       "      <td>0.916373</td>\n",
       "      <td>{'countvectorizer__max_features': 6700, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.12}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.939609</td>\n",
       "      <td>0.912985</td>\n",
       "      <td>0.938581</td>\n",
       "      <td>0.911462</td>\n",
       "      <td>{'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.942982</td>\n",
       "      <td>0.905902</td>\n",
       "      <td>0.942322</td>\n",
       "      <td>0.904932</td>\n",
       "      <td>{'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.969186</td>\n",
       "      <td>0.904216</td>\n",
       "      <td>0.968182</td>\n",
       "      <td>0.902283</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.927238</td>\n",
       "      <td>0.903204</td>\n",
       "      <td>0.925733</td>\n",
       "      <td>0.901447</td>\n",
       "      <td>{'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>minimal</td>\n",
       "      <td>no</td>\n",
       "      <td>0.968840</td>\n",
       "      <td>0.902055</td>\n",
       "      <td>0.967164</td>\n",
       "      <td>0.898785</td>\n",
       "      <td>{'logisticregression__C': 0.001, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>minimal</td>\n",
       "      <td>no</td>\n",
       "      <td>0.959632</td>\n",
       "      <td>0.902055</td>\n",
       "      <td>0.956994</td>\n",
       "      <td>0.897909</td>\n",
       "      <td>{'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.974247</td>\n",
       "      <td>0.898482</td>\n",
       "      <td>0.973243</td>\n",
       "      <td>0.896592</td>\n",
       "      <td>{'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.911493</td>\n",
       "      <td>0.897470</td>\n",
       "      <td>0.910347</td>\n",
       "      <td>0.895530</td>\n",
       "      <td>{'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.956028</td>\n",
       "      <td>0.897133</td>\n",
       "      <td>0.953674</td>\n",
       "      <td>0.893344</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_cleaning lemmatized  train_acc  test_acc  train_bal_acc  test_bal_acc  \\\n",
       "2       minimal         no   0.937681  0.917148       0.937164      0.916373   \n",
       "8          deep        yes   0.939609  0.912985       0.938581      0.911462   \n",
       "9          deep        yes   0.942982  0.905902       0.942322      0.904932   \n",
       "4          deep        yes   0.969186  0.904216       0.968182      0.902283   \n",
       "7          deep         no   0.927238  0.903204       0.925733      0.901447   \n",
       "1       minimal         no   0.968840  0.902055       0.967164      0.898785   \n",
       "0       minimal         no   0.959632  0.902055       0.956994      0.897909   \n",
       "5          deep        yes   0.974247  0.898482       0.973243      0.896592   \n",
       "6          deep         no   0.911493  0.897470       0.910347      0.895530   \n",
       "3          deep         no   0.956028  0.897133       0.953674      0.893344   \n",
       "\n",
       "                                                                                                                                                       params  \n",
       "2     {'countvectorizer__max_features': 6700, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.12}  \n",
       "8     {'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}  \n",
       "9      {'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "4        {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}  \n",
       "7     {'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "1        {'logisticregression__C': 0.001, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None}  \n",
       "0        {'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.001}  \n",
       "5   {'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "6      {'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}  \n",
       "3  {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ranking all models\n",
    "\n",
    "df.sort_values(by='test_bal_acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd61c1-337f-4123-bbb0-817434206c7f",
   "metadata": {},
   "source": [
    "**It looks like the Bayes model (index 2) with minimal cleaning and no lemmatization had the best test accuracy score (.917) and test balanced accuracy score (.916) among all models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778c7e3-4287-4494-b305-cea4704af592",
   "metadata": {},
   "source": [
    "### Now looking at only models created after deep cleaning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7640b15-f2dd-42e2-86b7-01d703412eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_cleaning</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>test_bal_acc</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.939609</td>\n",
       "      <td>0.912985</td>\n",
       "      <td>0.938581</td>\n",
       "      <td>0.911462</td>\n",
       "      <td>{'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.942982</td>\n",
       "      <td>0.905902</td>\n",
       "      <td>0.942322</td>\n",
       "      <td>0.904932</td>\n",
       "      <td>{'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.969186</td>\n",
       "      <td>0.904216</td>\n",
       "      <td>0.968182</td>\n",
       "      <td>0.902283</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.927238</td>\n",
       "      <td>0.903204</td>\n",
       "      <td>0.925733</td>\n",
       "      <td>0.901447</td>\n",
       "      <td>{'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.974247</td>\n",
       "      <td>0.898482</td>\n",
       "      <td>0.973243</td>\n",
       "      <td>0.896592</td>\n",
       "      <td>{'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.911493</td>\n",
       "      <td>0.897470</td>\n",
       "      <td>0.910347</td>\n",
       "      <td>0.895530</td>\n",
       "      <td>{'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.956028</td>\n",
       "      <td>0.897133</td>\n",
       "      <td>0.953674</td>\n",
       "      <td>0.893344</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_cleaning lemmatized  train_acc  test_acc  train_bal_acc  test_bal_acc  \\\n",
       "8          deep        yes   0.939609  0.912985       0.938581      0.911462   \n",
       "9          deep        yes   0.942982  0.905902       0.942322      0.904932   \n",
       "4          deep        yes   0.969186  0.904216       0.968182      0.902283   \n",
       "7          deep         no   0.927238  0.903204       0.925733      0.901447   \n",
       "5          deep        yes   0.974247  0.898482       0.973243      0.896592   \n",
       "6          deep         no   0.911493  0.897470       0.910347      0.895530   \n",
       "3          deep         no   0.956028  0.897133       0.953674      0.893344   \n",
       "\n",
       "                                                                                                                                                       params  \n",
       "8     {'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}  \n",
       "9      {'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "4        {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}  \n",
       "7     {'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "5   {'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "6      {'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}  \n",
       "3  {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sorting by accuracy score of test data\n",
    "\n",
    "df[df['data_cleaning']=='deep'].sort_values(by='test_acc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4df6a62-89b7-407b-9c23-80be6c31e54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_cleaning</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>test_bal_acc</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.939609</td>\n",
       "      <td>0.912985</td>\n",
       "      <td>0.938581</td>\n",
       "      <td>0.911462</td>\n",
       "      <td>{'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.942982</td>\n",
       "      <td>0.905902</td>\n",
       "      <td>0.942322</td>\n",
       "      <td>0.904932</td>\n",
       "      <td>{'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.969186</td>\n",
       "      <td>0.904216</td>\n",
       "      <td>0.968182</td>\n",
       "      <td>0.902283</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.927238</td>\n",
       "      <td>0.903204</td>\n",
       "      <td>0.925733</td>\n",
       "      <td>0.901447</td>\n",
       "      <td>{'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.974247</td>\n",
       "      <td>0.898482</td>\n",
       "      <td>0.973243</td>\n",
       "      <td>0.896592</td>\n",
       "      <td>{'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.911493</td>\n",
       "      <td>0.897470</td>\n",
       "      <td>0.910347</td>\n",
       "      <td>0.895530</td>\n",
       "      <td>{'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.956028</td>\n",
       "      <td>0.897133</td>\n",
       "      <td>0.953674</td>\n",
       "      <td>0.893344</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_cleaning lemmatized  train_acc  test_acc  train_bal_acc  test_bal_acc  \\\n",
       "8          deep        yes   0.939609  0.912985       0.938581      0.911462   \n",
       "9          deep        yes   0.942982  0.905902       0.942322      0.904932   \n",
       "4          deep        yes   0.969186  0.904216       0.968182      0.902283   \n",
       "7          deep         no   0.927238  0.903204       0.925733      0.901447   \n",
       "5          deep        yes   0.974247  0.898482       0.973243      0.896592   \n",
       "6          deep         no   0.911493  0.897470       0.910347      0.895530   \n",
       "3          deep         no   0.956028  0.897133       0.953674      0.893344   \n",
       "\n",
       "                                                                                                                                                       params  \n",
       "8     {'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}  \n",
       "9      {'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "4        {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}  \n",
       "7     {'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "5   {'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "6      {'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}  \n",
       "3  {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sorting by balanced accuracy score of test data\n",
    "\n",
    "df[df['data_cleaning']=='deep'].sort_values(by='test_bal_acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def707d-5f58-4cf7-942f-5490c64142c4",
   "metadata": {},
   "source": [
    "***Looks like the order of highest to lowest scores is same for both metrics! The best model uses CountVectorizer and Bayes with the following parameters:***\n",
    "\n",
    "- CountVectorizer: \n",
    "    - Maximum Features: 5,000\n",
    "    - Ngram Range: (1,1)\n",
    "    - Removing English stop words\n",
    "- Bayes Multinomial:\n",
    "    - Alpha:0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91acbb-dbfd-453f-b895-3e8fa44d939c",
   "metadata": {},
   "source": [
    "I would like to explore the different models more (tfdif vs count, lemmatized vs non-lemmatized). and in the interest of time, I edited the summary file and added couple of columns (estimators, vectorizer). I will read in this file and do my exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a967da0b-376b-4236-99a6-d9ecf0b692f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.read_csv('../datasets/model_list_summary-final-with-additions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2ba2006-050b-4b27-8cbc-d7a283b76e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>data_cleaning</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>test_bal_acc</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logreg</td>\n",
       "      <td>count</td>\n",
       "      <td>minimal</td>\n",
       "      <td>no</td>\n",
       "      <td>0.959632</td>\n",
       "      <td>0.902055</td>\n",
       "      <td>0.956994</td>\n",
       "      <td>0.897909</td>\n",
       "      <td>{'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logreg</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>minimal</td>\n",
       "      <td>no</td>\n",
       "      <td>0.968840</td>\n",
       "      <td>0.902055</td>\n",
       "      <td>0.967164</td>\n",
       "      <td>0.898785</td>\n",
       "      <td>{'logisticregression__C': 0.001, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  estimator vectorizer data_cleaning lemmatized  train_acc  test_acc  \\\n",
       "0    logreg      count       minimal         no   0.959632  0.902055   \n",
       "1    logreg      tfidf       minimal         no   0.968840  0.902055   \n",
       "\n",
       "   train_bal_acc  test_bal_acc  \\\n",
       "0       0.956994      0.897909   \n",
       "1       0.967164      0.898785   \n",
       "\n",
       "                                                                                                                                                 params  \n",
       "0  {'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.001}  \n",
       "1  {'logisticregression__C': 0.001, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None}  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0729432b-c406-4ec8-857d-287cdf8934dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>data_cleaning</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>test_bal_acc</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logreg</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.969186</td>\n",
       "      <td>0.904216</td>\n",
       "      <td>0.968182</td>\n",
       "      <td>0.902283</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logreg</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.974247</td>\n",
       "      <td>0.898482</td>\n",
       "      <td>0.973243</td>\n",
       "      <td>0.896592</td>\n",
       "      <td>{'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logreg</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.956028</td>\n",
       "      <td>0.897133</td>\n",
       "      <td>0.953674</td>\n",
       "      <td>0.893344</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  estimator vectorizer data_cleaning lemmatized  train_acc  test_acc  \\\n",
       "4    logreg      count          deep        yes   0.969186  0.904216   \n",
       "5    logreg      tfidf          deep        yes   0.974247  0.898482   \n",
       "3    logreg      count          deep         no   0.956028  0.897133   \n",
       "\n",
       "   train_bal_acc  test_bal_acc  \\\n",
       "4       0.968182      0.902283   \n",
       "5       0.973243      0.896592   \n",
       "3       0.953674      0.893344   \n",
       "\n",
       "                                                                                                                                                       params  \n",
       "4        {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}  \n",
       "5   {'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "3  {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models[ (df_models['data_cleaning']=='deep') & (df_models['estimator']=='logreg')].sort_values(by='test_bal_acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c76ad-c6e9-42d6-83fb-a1fa723d34b7",
   "metadata": {},
   "source": [
    "The table above indicates that with the deep cleaning, and using logistic regression as the estimator, the balanced acccuracy score for the test set is best with CountVectorizer with lemmatization, followed by TfidfVectorizer with lemmatization. Lemmatization gave better models than non-lemmatization, and CountVectorization is better than TfidfVectorizer. (Using balanced accuracy score on test data as the metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a27a53c-3721-461f-a1b0-61b49321aac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>data_cleaning</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>test_bal_acc</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bayes</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.939609</td>\n",
       "      <td>0.912985</td>\n",
       "      <td>0.938581</td>\n",
       "      <td>0.911462</td>\n",
       "      <td>{'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bayes</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.942982</td>\n",
       "      <td>0.905902</td>\n",
       "      <td>0.942322</td>\n",
       "      <td>0.904932</td>\n",
       "      <td>{'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bayes</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.927238</td>\n",
       "      <td>0.903204</td>\n",
       "      <td>0.925733</td>\n",
       "      <td>0.901447</td>\n",
       "      <td>{'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bayes</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.911493</td>\n",
       "      <td>0.897470</td>\n",
       "      <td>0.910347</td>\n",
       "      <td>0.895530</td>\n",
       "      <td>{'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  estimator vectorizer data_cleaning lemmatized  train_acc  test_acc  \\\n",
       "8     bayes      count          deep        yes   0.939609  0.912985   \n",
       "9     bayes      tfidf          deep        yes   0.942982  0.905902   \n",
       "7     bayes      tfidf          deep         no   0.927238  0.903204   \n",
       "6     bayes      count          deep         no   0.911493  0.897470   \n",
       "\n",
       "   train_bal_acc  test_bal_acc  \\\n",
       "8       0.938581      0.911462   \n",
       "9       0.942322      0.904932   \n",
       "7       0.925733      0.901447   \n",
       "6       0.910347      0.895530   \n",
       "\n",
       "                                                                                                                                                    params  \n",
       "8  {'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}  \n",
       "9   {'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "7  {'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "6   {'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models[ (df_models['data_cleaning']=='deep') & (df_models['estimator']=='bayes')].sort_values(by='test_bal_acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a72dc0-de3e-4625-abf3-5cfca2d8fa20",
   "metadata": {},
   "source": [
    "Like with logistic regression, doing lemmatization with Bayes estimator also produces better model as indicated by the table above.  With lemmatization, CountVectorizer performed better than TfidfVectorizer, but withouth lemmatization, TfidfVectorizer performed better than CountVectorizer. (Using balanced accuracy score on test data as the metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b3e5466-aafa-4ce2-95e9-e24aae99562e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>data_cleaning</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>test_bal_acc</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bayes</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.942982</td>\n",
       "      <td>0.905902</td>\n",
       "      <td>0.942322</td>\n",
       "      <td>0.904932</td>\n",
       "      <td>{'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bayes</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.927238</td>\n",
       "      <td>0.903204</td>\n",
       "      <td>0.925733</td>\n",
       "      <td>0.901447</td>\n",
       "      <td>{'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logreg</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.974247</td>\n",
       "      <td>0.898482</td>\n",
       "      <td>0.973243</td>\n",
       "      <td>0.896592</td>\n",
       "      <td>{'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  estimator vectorizer data_cleaning lemmatized  train_acc  test_acc  \\\n",
       "9     bayes      tfidf          deep        yes   0.942982  0.905902   \n",
       "7     bayes      tfidf          deep         no   0.927238  0.903204   \n",
       "5    logreg      tfidf          deep        yes   0.974247  0.898482   \n",
       "\n",
       "   train_bal_acc  test_bal_acc  \\\n",
       "9       0.942322      0.904932   \n",
       "7       0.925733      0.901447   \n",
       "5       0.973243      0.896592   \n",
       "\n",
       "                                                                                                                                                      params  \n",
       "9     {'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "7    {'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "5  {'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models[ (df_models['data_cleaning']=='deep') & (df_models['vectorizer']=='tfidf')].sort_values(by='test_bal_acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82a0e7-e5ac-4383-a6a8-b450d9504b22",
   "metadata": {},
   "source": [
    "The table above indicates that TfidfVectorizer works best with Bayes estimator, regardless of whether data is lemmatized or not, than Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d23e37fa-20e8-40cf-af44-a82d684c3cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>data_cleaning</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>test_bal_acc</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bayes</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.939609</td>\n",
       "      <td>0.912985</td>\n",
       "      <td>0.938581</td>\n",
       "      <td>0.911462</td>\n",
       "      <td>{'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logreg</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.969186</td>\n",
       "      <td>0.904216</td>\n",
       "      <td>0.968182</td>\n",
       "      <td>0.902283</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bayes</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.911493</td>\n",
       "      <td>0.897470</td>\n",
       "      <td>0.910347</td>\n",
       "      <td>0.895530</td>\n",
       "      <td>{'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logreg</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.956028</td>\n",
       "      <td>0.897133</td>\n",
       "      <td>0.953674</td>\n",
       "      <td>0.893344</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  estimator vectorizer data_cleaning lemmatized  train_acc  test_acc  \\\n",
       "8     bayes      count          deep        yes   0.939609  0.912985   \n",
       "4    logreg      count          deep        yes   0.969186  0.904216   \n",
       "6     bayes      count          deep         no   0.911493  0.897470   \n",
       "3    logreg      count          deep         no   0.956028  0.897133   \n",
       "\n",
       "   train_bal_acc  test_bal_acc  \\\n",
       "8       0.938581      0.911462   \n",
       "4       0.968182      0.902283   \n",
       "6       0.910347      0.895530   \n",
       "3       0.953674      0.893344   \n",
       "\n",
       "                                                                                                                                                       params  \n",
       "8     {'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}  \n",
       "4        {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}  \n",
       "6      {'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}  \n",
       "3  {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models[ (df_models['data_cleaning']=='deep') & (df_models['vectorizer']=='count')].sort_values(by='test_bal_acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a9130-4ef6-470f-8c77-f041152a9edb",
   "metadata": {},
   "source": [
    "The table above indicates that CountVectorizer works best when data is lemmatized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa30997e-e104-467c-8e32-fd4a275ae9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>data_cleaning</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>test_bal_acc</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bayes</td>\n",
       "      <td>count</td>\n",
       "      <td>minimal</td>\n",
       "      <td>no</td>\n",
       "      <td>0.937681</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>0.937164</td>\n",
       "      <td>0.916373</td>\n",
       "      <td>{'countvectorizer__max_features': 6700, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.12}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bayes</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.939609</td>\n",
       "      <td>0.912985</td>\n",
       "      <td>0.938581</td>\n",
       "      <td>0.911462</td>\n",
       "      <td>{'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bayes</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.942982</td>\n",
       "      <td>0.905902</td>\n",
       "      <td>0.942322</td>\n",
       "      <td>0.904932</td>\n",
       "      <td>{'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logreg</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.969186</td>\n",
       "      <td>0.904216</td>\n",
       "      <td>0.968182</td>\n",
       "      <td>0.902283</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bayes</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.927238</td>\n",
       "      <td>0.903204</td>\n",
       "      <td>0.925733</td>\n",
       "      <td>0.901447</td>\n",
       "      <td>{'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logreg</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>minimal</td>\n",
       "      <td>no</td>\n",
       "      <td>0.968840</td>\n",
       "      <td>0.902055</td>\n",
       "      <td>0.967164</td>\n",
       "      <td>0.898785</td>\n",
       "      <td>{'logisticregression__C': 0.001, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logreg</td>\n",
       "      <td>count</td>\n",
       "      <td>minimal</td>\n",
       "      <td>no</td>\n",
       "      <td>0.959632</td>\n",
       "      <td>0.902055</td>\n",
       "      <td>0.956994</td>\n",
       "      <td>0.897909</td>\n",
       "      <td>{'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logreg</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>deep</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.974247</td>\n",
       "      <td>0.898482</td>\n",
       "      <td>0.973243</td>\n",
       "      <td>0.896592</td>\n",
       "      <td>{'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bayes</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.911493</td>\n",
       "      <td>0.897470</td>\n",
       "      <td>0.910347</td>\n",
       "      <td>0.895530</td>\n",
       "      <td>{'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logreg</td>\n",
       "      <td>count</td>\n",
       "      <td>deep</td>\n",
       "      <td>no</td>\n",
       "      <td>0.956028</td>\n",
       "      <td>0.897133</td>\n",
       "      <td>0.953674</td>\n",
       "      <td>0.893344</td>\n",
       "      <td>{'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  estimator vectorizer data_cleaning lemmatized  train_acc  test_acc  \\\n",
       "2     bayes      count       minimal         no   0.937681  0.917148   \n",
       "8     bayes      count          deep        yes   0.939609  0.912985   \n",
       "9     bayes      tfidf          deep        yes   0.942982  0.905902   \n",
       "4    logreg      count          deep        yes   0.969186  0.904216   \n",
       "7     bayes      tfidf          deep         no   0.927238  0.903204   \n",
       "1    logreg      tfidf       minimal         no   0.968840  0.902055   \n",
       "0    logreg      count       minimal         no   0.959632  0.902055   \n",
       "5    logreg      tfidf          deep        yes   0.974247  0.898482   \n",
       "6     bayes      count          deep         no   0.911493  0.897470   \n",
       "3    logreg      count          deep         no   0.956028  0.897133   \n",
       "\n",
       "   train_bal_acc  test_bal_acc  \\\n",
       "2       0.937164      0.916373   \n",
       "8       0.938581      0.911462   \n",
       "9       0.942322      0.904932   \n",
       "4       0.968182      0.902283   \n",
       "7       0.925733      0.901447   \n",
       "1       0.967164      0.898785   \n",
       "0       0.956994      0.897909   \n",
       "5       0.973243      0.896592   \n",
       "6       0.910347      0.895530   \n",
       "3       0.953674      0.893344   \n",
       "\n",
       "                                                                                                                                                       params  \n",
       "2     {'countvectorizer__max_features': 6700, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.12}  \n",
       "8     {'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.07}  \n",
       "9      {'multinomialnb__alpha': 0.3, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "4        {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.003}  \n",
       "7     {'multinomialnb__alpha': 0.05, 'tfidfvectorizer__max_features': 3300, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "1        {'logisticregression__C': 0.001, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None}  \n",
       "0        {'countvectorizer__max_features': 5000, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None, 'logisticregression__C': 0.001}  \n",
       "5   {'logisticregression__C': 0.003, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': 'english'}  \n",
       "6      {'countvectorizer__max_features': 3000, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'multinomialnb__alpha': 0.3}  \n",
       "3  {'countvectorizer__max_features': 4500, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 0.0008}  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ranking all models again\n",
    "\n",
    "df_models.sort_values(by='test_bal_acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82813c8b-5e69-44b3-93e3-084c117d94cf",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "Based on the exploration and modeling of the data, below are my findings and recommendations:\n",
    "\n",
    "1. Use Bayes model. The Bayes models performed better and had a higher balanced accuracy score better than logistic regression models.\n",
    "2. Be discerning in data cleaning. As we see in the model scores created in this project, more data cleaning does not necessarily mean better models.\n",
    "3. Lemmatize. For both Bayes and Logistic Regression, lemmatization brought up the balanced accuracy score. \n",
    "4. Consider both CountVectorizer and TfidfVectorizer since they both vary in performance depending on the estimators, lemmatization, and other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0162397-b8ab-484b-a04e-7793130820c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
